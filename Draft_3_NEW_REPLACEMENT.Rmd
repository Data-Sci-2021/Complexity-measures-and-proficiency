---
title: "Draft_3_NEW_REPLACEMENT"
author: "Rossina Soyan"
date: "12/2/2021"
output:   
  github_document: 
    toc: TRUE
---

```{r setup}
##Set knitr options (show both code and output, show output w/o leading #, make figures smaller, hold figures until after chunk)
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment=NA, fig.height=3, fig.width=4.2, fig.show="hold")
Sys.setlocale("LC_CTYPE", "Russian") #to make sure my text is not gibberish, readable
```

## What I need to load to run the codes?

```{r}
library(tidyverse)
#install.packages('quanteda') to work with a corpus
library(quanteda)
#install.packages("htmlwidgets") to work with strings and regular expressions
library(htmlwidgets)
# install.packages('tidytext') to be able to perform tokenization
library(tidytext)
```

## Upload the corpus, create a dataframe with texts and names

I have decided to upload texts written by only 8 students because I am running out of time. I have chosen 4 students rated as Intermediate and 4 students rated as Advanced. Each student has submitted 3 texts as part of their placement examination, it means that the current corpus consists of 24 texts.

```{r}
#How to upload the texts
#From https://www.youtube.com/watch?v=pFinlXYLZ-A
folder <- "C:/Users/Rossina/Documents/CMU_student/3_Fall_2021/Statistics_at_Pitt/data"
filelist_orig <- list.files(path = folder, pattern = ".*.txt")
filelist <- paste(folder, "/", filelist_orig, sep = "")
filelistTexts <- lapply(filelist, FUN = readLines, encoding = "UTF-8", warn = FALSE)
texts <- lapply(filelistTexts, FUN = paste, collapse = " ") %>% 
  str_remove_all("unclear") #I deleted the word "unclear" from the texts
```


Now that I have all the texts uploaded, I want to create a dataframe with texts and names

```{r}
class(texts) #Interesting! It used to show that this is a list, but now the answer is "character"
corpus_df <- tibble(text = texts, Student = filelist_orig)
corpus_df #Success
```
I need to separate student ids and student texts

```{r}
corpus_df2 <- corpus_df %>% 
  separate(Student, c("Student", "Entry"))
corpus_df2
```

I need to add their proficiency ratings

```{r}
#I am using something I saw in Winter's textbook + https://rstudio-pubs-static.s3.amazonaws.com/116317_e6922e81e72e4e3f83995485ce686c14.html#/9
corpus_df3 <- mutate(corpus_df2, Proficiency = ifelse(grepl("[1-4]", Student), "Intermediate", "Advanced"))
head(corpus_df3) #Success!
```
I need to tokenize my texts

```{r}
corpus_df_tidy <- corpus_df3 %>% 
  mutate(text = gsub(x = text, pattern = "\\-\\s", replacement = "")) %>% #to make sure there are no lonely dashes as token
  unnest_tokens(word, text, token = "regex", pattern = "[\\s,\\.\\?!\\(\\)\\:\";]") #let me check if words look like real words in all these 8 texts
corpus_df_tidy %>% 
  filter(str_detect(word, "-")) #Success! 60 words have been identified as 1 word, not two. That's great!
```

## Lexical complexity measures

### Lexical density 

These files are the lists of non-lexical words that I have created

```{r}
RusConjCoord2 <- readLines("additional_documents/Russian_conjunctions_COORD.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>") 
RusPrep2 <- readLines("additional_documents/Russian_prepositions.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>") 
RusConjSubord2 <- readLines("additional_documents/Russian_conjunctions_SUBORD.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>")
RusInter2 <- readLines("additional_documents/Russian_interjections.txt", encoding = "UTF-8", warn = FALSE) %>%
  str_remove_all("<.+>")
RusPrtcl2 <- readLines("additional_documents/Russian_particles.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>") 
AllnonLEX <- c(RusConjCoord2, RusPrep2, RusConjSubord2, RusInter2, RusPrtcl2) 
AllnonLEX_df <- tibble(word = AllnonLEX) #This column name should be the same for using the anti-join()
head(AllnonLEX_df) #Success! 
```
And now I need to calculate lexical density 

```{r}
df_LexDens_sep <-corpus_df_tidy %>% 
  group_by(Student, Entry) %>% 
  summarize(total_words = n(),
         lexical_words = sum(!word %in% AllnonLEX_df$word),
         lexical_density = lexical_words/total_words)

df_LexDens_sep
```
Manual check

```{r}
316+278+282 #matches
(0.7246835 + 0.7877698 + 0.7517730)/3  #  The number here 0.7547421 while the number below is 0.7534247. I need to think about these two numbers
```


I want to compare the averaged numbers

```{r}
df_LexDens_tog <-corpus_df_tidy %>% 
  group_by(Student) %>% 
  summarize(total_words = n(),
         lexical_words = sum(!word %in% AllnonLEX_df$word),
         lexical_density = lexical_words/total_words)

df_LexDens_tog
```

### Lexial variation

Lexial variation is measured with type-token ratio but since TTR is affected by text length, researchers started using MTLD instead.

```{r}
#I googled MTLD function in the koRpus package. Let's try it out.
#install.packages("koRpus")
#install.koRpus.lang(c("en","ru"))
#available.koRpus.lang()
library(koRpus) #I hope this package helps me calculate MTLD
library(koRpus.lang.ru)
library(koRpus.lang.en)
#I am expected to tokenize the text within the tokenize function of this package

#My failed attempd to calculate MTLD for all entries and students in one fell swoop. The results is gibberish
Tokens_for_MTLD <- lapply(filelist, FUN = tokenize, fileEncoding = "UTF-8", lang = "ru") 
#I still don't know how to filter out the word "unclear"
lapply(Tokens_for_MTLD, FUN = MTLD) #Success. The MTLD for S1_E1 is 175.36. Let's check manually

#I am checking the MTLD manually
S1_E1 <- tokenize("C:/Users/Rossina/Documents/CMU_student/3_Fall_2021/Statistics_at_Pitt/data/Student1_Entry1.txt", fileEncoding = "UTF-8", lang = "ru")  #Success. 
#I tried using filter to remove "unclear" and str_remove_all - it didn't work out. I cannot even find out if there are words "unclear" in the text
#S1_E1 %>% 
#  filter(str_detect(token, "unclear"))
MTLD(S1_E1) #Success! The number matches the number from above - 175.36

#I am not sure how to put the results into a tibble but NOT manually. The results look like this
#Language: "ru"

#Total number of tokens: 320 
#Total number of types:  204

#Measure of Textual Lexical Diversity
 #             MTLD: 175.36 
 #Number of factors: NA 
  #     Factor size: 0.72 
  #SD tokens/factor: 59.12 (all factors) 
   #                 70.71 (complete factors only)

#Note: Analysis was conducted case insensitive.
```

### Lexical sophistication

Lexical sophistication is measured using the average word length. I need to calculate the length of each word, add everything up and divide by the number of words in each text. 

```{r}
AWL_sep <-corpus_df_tidy %>% 
  group_by(Student, Entry) %>% 
  summarize(total_words = n(),
         total_word_length = sum(str_length(word)),
         AWL = total_word_length/total_words)
AWL_sep #Success! 
```

```{r}
AWL_tog <-corpus_df_tidy %>% 
  group_by(Student) %>% 
  summarize(total_words = n(),
         total_word_length = sum(str_length(word)),
         AWL = total_word_length/total_words)
AWL_tog #Success! Seems acceptable
```

## A dataframe with all the findings 

I need to input my students' names, their proficiency and numbers for their lexical complexity measures

```{r}
df_for_clustering <- corpus_df3 %>% 
  full_join(df_LexDens_sep) %>% 
  full_join(AWL_sep) #I don't know how to create an MTLD tibble NOT manually yet
```

## An attempt to do the cluster analysis (no progress yet)

I am not sure where to begin. What I want to do is to see if lexical density, lexical sophistication, and lexical variation produce a cluster typical for Intermediate or Advanced proficiency levels, how lexical complexity measures match the proficiency levels. I want to conduct hierarchical cluster analysis similar to Jarvis et al.(2003). They had more data points and linguistic features, but my project is a pilot project.  

```{r}
#From https://www.datacamp.com/community/tutorials/hierarchical-clustering-R
#I need to scale my data points
```

# Final notes
```{r}
Sys.setlocale("LC_CTYPE", "English") # to make sure that nothing changes how my other files are read
```

## Session info


```{r}
sessionInfo()
```
