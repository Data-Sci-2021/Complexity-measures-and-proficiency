---
title: "Draft_3_NEW_REPLACEMENT"
author: "Rossina Soyan"
date: "12/2/2021"
output:   
  github_document: 
    toc: TRUE
---

```{r setup}
##Set knitr options (show both code and output, show output w/o leading #, make figures smaller, hold figures until after chunk)
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment=NA, fig.height=3, fig.width=4.2, fig.show="hold")
Sys.setlocale("LC_CTYPE", "Russian") #to make sure my text is not gibberish, readable
```

## What I need to load to run the codes?

```{r}
library(tidyverse)
#install.packages('quanteda') to work with a corpus
library(quanteda)
#install.packages("htmlwidgets") to work with strings and regular expressions
library(htmlwidgets)
# install.packages('tidytext') to be able to perform tokenization
library(tidytext)
```

## Upload the corpus, create a dataframe with texts and names

I have decided to upload texts written by only 8 students because I am running out of time. I have chosen 4 students rated as Intermediate and 4 students rated as Advanced. Each student has submitted 3 texts as part of their placement examination, it means that the current corpus consists of 24 texts.

```{r}
#How to upload the texts
#From https://www.youtube.com/watch?v=pFinlXYLZ-A
folder <- "C:/Users/Rossina/Documents/CMU_student/3_Fall_2021/Statistics_at_Pitt/data"
filelist_orig <- list.files(path = folder, pattern = ".*.txt")
filelist <- paste(folder, "/", filelist_orig, sep = "")
filelistTexts <- lapply(filelist, FUN = readLines, encoding = "UTF-8", warn = FALSE)
texts <- lapply(filelistTexts, FUN = paste, collapse = " ") %>% 
  str_remove_all("unclear") #I deleted the word "unclear" from the texts
```


Now that I have all the texts uploaded, I want to create a dataframe with texts and names

```{r}
class(texts) #Interesting! It used to show that this is a list, but now the answer is "character"
corpus_df <- tibble(text = texts, Student = filelist_orig)
corpus_df #Success
```
I need to separate student ids and student texts

```{r}
corpus_df2 <- corpus_df %>% 
  separate(Student, c("Student", "Entry"))
corpus_df2
```

I need to add their proficiency ratings

```{r}
#I am using something I saw in Winter's textbook + https://rstudio-pubs-static.s3.amazonaws.com/116317_e6922e81e72e4e3f83995485ce686c14.html#/9
corpus_df3 <- mutate(corpus_df2, Proficiency = ifelse(grepl("[1-4]", Student), "Intermediate", "Advanced"))
head(corpus_df3) #Success!
```
I need to tokenize my texts

```{r}
corpus_df_tidy <- corpus_df3 %>% 
  mutate(text = gsub(x = text, pattern = "\\-\\s", replacement = "")) %>% #to make sure there are no lonely dashes as token
  unnest_tokens(word, text, token = "regex", pattern = "[\\s,\\.\\?!\\(\\)\\:\";]") #let me check if words look like real words in all these 8 texts
corpus_df_tidy %>% 
  filter(str_detect(word, "-")) #Success! 60 words have been identified as 1 word, not two. That's great!
```

## Lexical complexity measures

### Lexical density 

These files are the lists of non-lexical words that I have created

```{r}
RusConjCoord2 <- readLines("additional_documents/Russian_conjunctions_COORD.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>") 
RusPrep2 <- readLines("additional_documents/Russian_prepositions.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>") 
RusConjSubord2 <- readLines("additional_documents/Russian_conjunctions_SUBORD.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>")
RusInter2 <- readLines("additional_documents/Russian_interjections.txt", encoding = "UTF-8", warn = FALSE) %>%
  str_remove_all("<.+>")
RusPrtcl2 <- readLines("additional_documents/Russian_particles.txt", encoding = "UTF-8", warn = FALSE) %>% 
  str_remove_all("<.+>") 
AllnonLEX <- c(RusConjCoord2, RusPrep2, RusConjSubord2, RusInter2, RusPrtcl2) 
AllnonLEX_df <- tibble(word = AllnonLEX) #This column name should be the same for using the anti-join()
head(AllnonLEX_df) #Success! 
```
And now I need to calculate lexical density 

```{r}
df_LexDens_sep <-corpus_df_tidy %>% 
  group_by(Student, Entry) %>% 
  summarize(total_words = n(),
         lexical_words = sum(!word %in% AllnonLEX_df$word),
         lexical_density = lexical_words/total_words)

df_LexDens_sep
```
Manual check

```{r}
316+278+282 #matches
(0.7246835 + 0.7877698 + 0.7517730)/3  #  Good enough
```


I want to compare the averaged numbers

```{r}
df_LexDens_tog <-corpus_df_tidy %>% 
  group_by(Student) %>% 
  summarize(total_words = n(),
         lexical_words = sum(!word %in% AllnonLEX_df$word),
         lexical_density = lexical_words/total_words)

df_LexDens_tog
```

### Lexial variation

Lexial variation is measured with type-token ratio but since TTR is affected by text length, researchers started using MTLD instead.

```{r}
#I googled MTLD function in the koRpus package. Let's try it out.
#install.packages("koRpus")
#install.koRpus.lang(c("en","ru"))
#available.koRpus.lang()
library(koRpus) #I hope this package helps me calculate MTLD
library(koRpus.lang.ru)
library(koRpus.lang.en)
#I am expected to tokenize the text within the tokenize function of this package

#My failed attempd to calculate MTLD for all entries and students in one fell swoop. The results is gibberish
Tokens_for_MTLD <- lapply(filelist, FUN = tokenize, fileEncoding = "UTF-8", lang = "ru") 
#I still don't know how to filter out the word "unclear"
lapply(Tokens_for_MTLD, FUN = MTLD) #Success. The MTLD for S1_E1 is 175.36. Let's check manually

#I am checking the MTLD manually
S1_E1 <- tokenize("C:/Users/Rossina/Documents/CMU_student/3_Fall_2021/Statistics_at_Pitt/data/Student1_Entry1.txt", fileEncoding = "UTF-8", lang = "ru")  #Success. 
#I tried using filter to remove "unclear" and str_remove_all - it didn't work out. I cannot even find out if there are words "unclear" in the text
#S1_E1 %>% 
#  filter(str_detect(token, "unclear"))
MTLD(S1_E1) #Success! The number matches the number from above - 175.36

#I am not sure how to put the results into a tibble but NOT manually. The results look like this
#Language: "ru"

#Total number of tokens: 320 
#Total number of types:  204

#Measure of Textual Lexical Diversity
 #             MTLD: 175.36 
 #Number of factors: NA 
  #     Factor size: 0.72 
  #SD tokens/factor: 59.12 (all factors) 
   #                 70.71 (complete factors only)

#Note: Analysis was conducted case insensitive.

filelist %>% 
  map(tokenize, fileEncoding = "UTF-8", lang = "ru") %>% 
  map(MTLD) %>% 
  map(attr, "MTLD") #it still spits out a lot of numbers. For now, I will just a vector manually, and I will think about figuring out the automatization later on.

MTLDs <- c(175.36, 210.55, 98.27, 152.57, 195.18, 65.09, 103.33, 135.09, 107.63, 195.88, 215.79, 126, 130.99, 189.28, 114.34, 132.16, 110.72, 212, 85.26, 116.43, 92.46, 138.16, 185.48, 157.07)

#Now I need to merge the vector with the student names and create a df with 8 rows
corpus_df4 <- corpus_df3 %>% 
  mutate(MTLD = MTLDs)

corpus_with_MTLDs_tog <- corpus_df4 %>% 
  group_by(Student) %>% 
  summarize(MTLD_tog = mean(MTLD))

corpus_with_MTLDs_tog

```

### Lexical sophistication

Lexical sophistication is measured using the average word length. I need to calculate the length of each word, add everything up and divide by the number of words in each text. 

```{r}
AWL_sep <-corpus_df_tidy %>% 
  group_by(Student, Entry) %>% 
  summarize(total_words = n(),
         total_word_length = sum(str_length(word)),
         AWL = total_word_length/total_words)
AWL_sep #Success! 
```

```{r}
AWL_tog <-corpus_df_tidy %>% 
  group_by(Student) %>% 
  summarize(total_words = n(),
         total_word_length = sum(str_length(word)),
         AWL = total_word_length/total_words)
AWL_tog #Success! Seems acceptable
```

## A dataframe with all the findings 

I need to input my students' names, their proficiency and numbers for their lexical complexity measures

```{r}
df_for_clustering_sep <- corpus_df3 %>% 
  full_join(df_LexDens_sep) %>% 
  full_join(AWL_sep) %>% 
  #I need to attach the MTLDs vector
  mutate(MTLD = MTLDs)

df_for_clustering_sep # I realized that student essays should be looked at together since their proficiency score is given based on their three essays together, not separately.

#I can look at the essays separately to see if a different trend will be revealed. What happens when we have 24 observations? This is not theoretically motivated but let's try

df_for_clustering_tog <- corpus_with_MTLDs_tog %>% 
  full_join(df_LexDens_tog) %>% 
  full_join(AWL_tog) 

df_for_clustering_tog 
  
```

## An attempt to do the cluster analysis 

### Attempt #1 with 8 observations

I want to do is to see if lexical density, lexical sophistication, and lexical variation produce a cluster typical for Intermediate or Advanced proficiency levels, how lexical complexity measures match the proficiency levels. I want to conduct hierarchical cluster analysis similar to Jarvis et al.(2003). They had more data points and linguistic features, but my project is a pilot project.  
From https://www.datacamp.com/community/tutorials/hierarchical-clustering-R
 - I need to scale my data points
 - Then, I can perform hierarchical cluster analysis
 - Last, I need to measure the goodness of clusters

```{r}
#I already have my dataframe df_for_clustering_tog
str(df_for_clustering_tog)
summary(df_for_clustering_tog)
any(is.na(df_for_clustering_tog)) #I knew my df pretty well but I checked it anyway
#I need proficiency information to check how good the clustering is at the end
Stud_Prof <- tibble(Student = df_for_clustering_tog$Student) %>% 
  mutate(Proficiency = ifelse(grepl("[1-4]", Student), "Intermediate", "Advanced"))
Stud_Prof_label <- Stud_Prof$Proficiency
```

The website tells me that I need to have all my numerical values scaled.

```{r}
#Actually, I need only three columns
df_three_columns <- df_for_clustering_tog %>% 
  select(MTLD_tog, lexical_density, AWL)
df_three_columns
#Now I am scaling the column values
df_cluster_sc <- as.data.frame(scale(df_three_columns))
summary(df_cluster_sc) #Mean for all the columns is 0 and the SD should be one
```

Now I need to build the distance matrix

```{r}
dist_mat <- dist(df_cluster_sc, method = 'euclidean') #Since all the values here are continuous numerical values, I need to use the euclidean distance method
#Following the website, I am trying out the average linkage method. The dendrogram is built via the hierarchical cluster object with the hclust() function.
hclust_avg <- hclust(dist_mat, method = 'average')
plot(hclust_avg)
```

I need to cut the dendogram into two clusters since I have two proficiency levels, Intermediate and Advanced. The cutree() function is used to cut the tree.

```{r}
cut_avg <- cutree(hclust_avg, k = 2)
#The illustration is not working. Maybe two clusters is too much of an imposition
#rect.hclust(hclust_avg , k = 2) #The rect.hclust() superimposes rectangular compartments for each cluster on the trr
#abline(h = 2, col = 'red') #The abline() function draws the cut line
```

Another attempt at illustration

```{r}
#install.packages('dendextend', dependencies = TRUE)
suppressPackageStartupMessages(library(dendextend))
avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, h = 2)
plot(avg_col_dend) #This looks nice
```

I am continuing doing what the website tells me to do.

```{r}
#The code below will help me to count how many observations are in each cluster. Since I have only 8 observations, this is not so important, but with hundreds of observations, this step would be extremely important.
texts_df_cl <- mutate(df_three_columns, cluster = cut_avg)
count(texts_df_cl, cluster) #This is an interesting answer. 
table(texts_df_cl$cluster,Stud_Prof_label) # 3 out of 8 observations were incorrectly clustered. I am not sure where to move from here.
```

### Attempt #2 
Let's see what will happen when we have 24 observations. Since the proficiency level depends on all three essays, this approach is not an accurate approach, but I will try it anyway.

```{r}
#Checking
str(df_for_clustering_sep)
summary(df_for_clustering_sep)
any(is.na(df_for_clustering_sep)) 
#I need proficiency information to check how good the clustering is at the end
Stud_Prof_label_sep <- df_for_clustering_sep$Proficiency 
#I need only three columns
cluster_sep_numerical <- df_for_clustering_sep %>% 
  select(AWL, MTLD, lexical_density)
cluster_sep_numerical
#Now I am scaling the column values
df_cluster_sc_sep <- as.data.frame(scale(cluster_sep_numerical))
summary(df_cluster_sc_sep) 
dist_mat_sep <- dist(df_cluster_sc_sep, method = 'euclidean') #Since all the values here are continuous numerical values, I need to use the euclidean distance method
#Following the website, I am trying out the average linkage method. The dendrogram is built via the hierarchical cluster object with the hclust() function.
hclust_avg_sep <- hclust(dist_mat_sep, method = 'average')
plot(hclust_avg_sep)
avg_dend_obj_sep <- as.dendrogram(hclust_avg_sep)
avg_col_dend_sep <- color_branches(avg_dend_obj_sep, h = 2)
plot(avg_col_dend_sep) #This looks interesting. Text 15 is apparently very different from the rest of texts
cut_avg_sep <- cutree(hclust_avg_sep, k = 2)
#Checking the model
texts_df_cl_sep <- mutate(cluster_sep_numerical, cluster = cut_avg_sep)
count(texts_df_cl_sep, cluster) #This is an interesting answer.
table(texts_df_cl_sep$cluster, Stud_Prof_label_sep) #This model is worse when everything is separate. What this tells me that lexical complexity measures of the texts in the corpus written by 8 students are very similar. MTLDs, AWLs, lexical density in these texts was not the defining characteristic in their rating. 
```


# Final notes
```{r}
Sys.setlocale("LC_CTYPE", "English") # to make sure that nothing changes how my other files are read
```

## Session info


```{r}
sessionInfo()
```
