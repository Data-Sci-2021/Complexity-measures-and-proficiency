---
title: "Draft_1"
author: "Rossina Soyan"
date: "10/25/2021"
output: 
  github_document: 
    toc: TRUE
---

```{r setup}
##Set knitr options (show both code and output, show output w/o leading #, make figures smaller, hold figures until after chunk)
knitr::opts_chunk$set(echo=TRUE, include=TRUE, comment=NA, fig.height=3, fig.width=4.2, fig.show="hold")
Sys.setlocale("LC_CTYPE", "Russian") #to make sure my text is not gibberish, readable

```

## First steps

```{r}
library(tidyverse)
#install.packages('readtext') to read a corpus
library(readtext)
#install.packages('quanteda') to work with a corpus
library(quanteda)
#install.packages("htmlwidgets") to work with strings and regular expressions
library(htmlwidgets)
# install.packages('tidytext') to be able to perform tokenization
library(tidytext)
#install.packages('tokenizers')
library(tokenizers)
```


## An attempt to read at least one .txt file

```{r}
(text1 <- readLines("data/text1.txt", encoding = "UTF-8"))
(tok_text1 <- tokenize_words(text1))
mod_text1 <- tibble(line = 1:145, word = tok_text1)
mod_text1
tok_text1 %>% filter("unclear")
(text1_df <- tibble(line = 1, text = text1)) #The text somehow becomes gibberish
tidy_text1 <- text1_df %>% 
  unnest_tokens(word, text, to_lower = FALSE) #word - tokenization unit, text - the name of the column with the text in text1
tidy_text1
```


I am using guidelines for readtext from this (source)[https://github.com/quanteda/readtext]

```{r}
text1 <- readtext("data/text1.txt", encoding = "UTF-8")
# I was able to load one text
text1$text
# I want to exclude the words "unclear" from the text
str_detect(text1$text, "unclear")
text1modified <- char_trim(text1$text, "sentences", exclude_pattern = "unclear")
text1modified #Unfortunately, this command excludes whole sentences containing the word unclear. And I want to only exclude the words "unclear."
# Let's try writing a function as in this link (https://stackoverflow.com/questions/35790652/removing-words-featured-in-character-vector-from-string)
stopwords = "unclear"
removeWords <- function(str, stopwords) {
  x <- unlist(strsplit(str, " "))
  paste(x[!x %in% stopwords], collapse = " ")
}
text1mod2 <- removeWords(text1$text, stopwords)
text1mod2 #success! I was able to remove the words "unclear" from the text in Russian.

```
## Tokenization

```{r}
# This is my attempt to tokenize the one text I loaded. Text1 is already a dataframe 
text1_tidy <- text1 %>% 
  unnest_tokens(word, text, to_lower = FALSE) #word - tokenization unit, text - the name of the column with the text in text1
#Now I have a tidy text but I have a column text in my df which I cannot delete
text1_tidy <- text1_tidy %>% 
  select(word)
text1_tidy #the spare column text is still haunting me
stopwords <-"unclear" # I will come up with other words later on
tidy_text1 <- anti_join(text1_tidy, by = "stopwords") #Why can't I remove the word "unclear"
text1_tidy #readtext seems to not work well with tidytext

```




## Complexity measures

Let me try to measure complexity measures in one text. By the way, since I am only working with one text right now, I can actually check the numbers manually.

## Global complexity - Mean sentence length

```{r}
str_length(text1$text) # This is the number of characters in this text with spaces
str_length(text1modified)
str_length(text1mod2) #Yes! It worked
#How do I count the number of sentences?
nsentence(text1$text)
nsentence(text1mod2) #the answers for both texts is 17 but I am not sure what to do with the following warning - Warning in nsentence.character(text1mod2) : nsentence() does not correctly count sentences in all lower-cased text. Can I just ignore it? Another issue is the address in the beginning of the text Дорогая Лара! This is not a real sentence. How do I not count it as a sentence?
#How do I count the number of words in a sentence? 
ntoken(text1$text, remove_punct = TRUE)
ntoken(text1mod2, remove_punct = TRUE) # Now it works!
(MSLtext1 <- ntoken(text1mod2, remove_punct = TRUE)/ nsentence(text1mod2)) # I have an answer except I do not know how to not count the address in the beginning of the text as not a sentence.
```

## Complexity by coordination - T-units per sentence

I am not sure how to count the number of T-units. I think I need to write my own function. If the definition of a sentence is anything which starts with an upper-case letter and ends with a period or ! or ?, then, the definition of a T-unit is a little bit more complex. Based on mt example test, T-unit may start with an upper- or lower-case and it can end before и if there is a subject and a verb after it or end with a period, ?, !.
```{r}
#nTunit <- function(str, )
```

## At the end of the document

```{r}
Sys.setlocale("LC_CTYPE", "English") # to make sure that nothing changes how my other files are read
```

