# Final report
## Rossina Soyan, Fall 2021, ros129@pitt.edu
## Introduction
### Summary
The goal of the project was to understand what lexical complexity measures correspond to intermediate and advanced proficiency levels in L2 Russian texts. To answer this question, I calculated three lexical complexity measures using a small sub-corpus of L2 Russian texts, performed a hierarchical cluster analysis and compared the results with the original proficiency levels. 

### Background/ Motivation
My interest in complexity measures in L2 Russian texts started in the summer of 2019 when I worked as a Russian language instructor at Middlebury Summer Russian School. One of the main elements of this language program is tracking students' progress using the ACTFL scale. After 8 weeks of intensive instruction, some students showed progress as defined by ACTFL, while other did not. What spurred my interest was the desire to see if certain lingusitic features correspond to the ACTFL proficiency levels. This line of research may contribute to the automatization of essay assessment in the future. 

### Theoretical background
In this project, L2 writing development is explored through the lens of lexical complexity and is understood as part of a larger framework of writing ability. While the narrow understanding of writing ability includes only grammatical knowledge and discourse aspects, the wider framework, the Writing Competence Model, also recognizes sociolinguistic and strategis competencies, as well as content and source use as essential (Barkaoui & Hadidi, 2020). Exploring all the aspects of the Writing Competence Model would be time-consuming and beyond the scope of the course, therefore, I decided to focus only on one aspect of grammatical competence, that is, lexical complexity (LC). 
LC is defined as the range of vocabulary used in a text (variation) and its sophistication (Wolfe-Quintero et al., 1998). LC is a multi-dimensional construct, however, its components are still in the process of negotiation. Another important piece of information to remember is that the majority of LC studies are based on L2 English texts. What linguistic features get to be quantified as part of LC in different languages may vary widely.
Most empirical studies tend to operationalize LC using two large-grained measures: lexical variation and lexical sophistication (Bulté & Housen, 2014; Higginbotham & Reid, 2019; Mazgutova & Kormos, 2015; Yoon & Polio, 2017). A new measure gaining in popularity is lexical bundles, or multi-word units (Barkaoui & Hadidi, 2020; Siyanova-Chanturia & Spina, 2020). In addition to these large-grained measures, researchers also investigate fine-grained measures. For example, Jarvis (2013, 2017) distinguishes between lexical diversity and lexical variability and identifies seven fine-grained measures within lexical diversity - volume, abundance, variety, evenness, dispersion, specialness, and disparity. Crossley et al. (2012) use the phrase L2 lexical competence instead of LC and understand it as consisting of vocabulary depth (measured through lexical diversity and lexical frequency indices), vocabulary breadth (measured through hypernymy, polysemy, semantic co-referentiality, and word associations), and accessibility of lexical items (measured through word concreteness, word familiarity, and word imageability). However, the tools for measuring these fine-grained measures are currently available only for L2 English texts.
In this project, I adopted Barkaoui and Hadidi's (2020) understanding of LC who distinguish four LC measures: lexical density, lexical variation, lexical sophistication, and lexical bundles. In order to measure lexical bundles, we need an additional, more representative corpus for comparison, which I do not have access to yet. Therefore, in this project, I decided to focus on the other three LC measures. *Lexical density* is understood as the ratio of lexical words to all words per essay. It is believed that the higher lexical density of essays, the higher the proficiency of students. *Lexical variation*, which is also known as lexical density, is operationalized as the ratio of the types to the tokens (TTR). A version of TTR less dependent on text length is Measure of Textual Lexical Diversity (MTLD). The hypothesis is the higher lexical variation of essays, the higher the proficiency of students. *Lexical sophistication* is understood as the proportion of relatively unusual, low-frequency word to frequent words in a text. In English, low-frequency words are longer than high-frequency words, therefore, lexical sophistication is sometimes calculated through average word length (AWL) by dividing the total number of letters by the total number of words. The larger the AWL of essays, the higher the proficiency of students.

## Dataset
To be able to finish the project on time, I only used a sub-corpus of texts from a bigger corpus of L2 Russian essays written by US college students. I randomly chose essays of eight students, four of whom were rated as intermediate and four as advanced. Each student was asked to write three essays as part of their placement in the language program. Therefore, my sub-corpus consists of 24 texts total. Students were expected to write their essays by hand without access to any additional resources. The time limit was 90 minutes. The ACTFL ratings were given by two trained Russian language intstructors. A third rater would be invited for the final rating if the first two raters did not agree on the proficiency level.
The handwritten essays were digitized by one RA and then checked by another RA. To make texts processable by udpipe, spelling was corrected in all the essays. 

## Overall history, or warts and all
My original goal for this final project was much more ambitious. I wanted to use the whole corpus which includes 601 texts and I wanted to measure not only lexical complexty but also syntactic complexity. From the outset, it turned out that every step would be painful. I started by loading one text and trying to figure out how to calculate each complexity measure. First of all, making sure that the computer loads the text in Cyrillic tunred out to be a challenge. I needed to set to the locale, make sure the encoding is specified. Every time I installed a new package, I needed to check if there was an additional package with the Russian language and install the additional package. Another problem was tokenization of hyphenated Russian words which were counted as two separate tokens. When I was done with lexical complexity measures, I tried to load spacyr and udpipe to see how the text would tagged. I figured out how to load the packages with the additional packages in Russian, but the output would have too many mistakes and I could not figure out how to count clauses, coordinated and subordinated. In the end, I decided to just focus on lexical complexity measures. The final problem was turning the output of the MTLD() function into a dataframe. My coding skills were not sufficient, so I gave up and transferred the numbers manually. Although the final result turned out to be much smaller than my original goals, I am still happy I practiced writing codes and working with the corpus data. 

## Analysis 
1. I loaded the sub-corpus to R. 
2. To calculate lexical density, I created five txt files with a possible list of non-lexical words in Russian. I anti-joined non-lexical words and lexical words in each essay. Then, I divided the number of lexical words to the total number of words. I came up with a lexical density number for each student based on their three essays.
3. To calculate lexical variation, I installed a package for calculating MTLD and calculated MTLD for all students. I had to create a vector for MTLD manually. 
4. To calculate lexical sophistication, I calculated the length of each word, added everything up and divided the total length by the total number of words. 
5. I conducted a hierarchical cluster analysis which defined as "a mathematical procedure for classifying cases (e.g., texts) into groups based on their shared similarities across a number of measures (e.g., linguistic features)” (Jarvis et al., 2003, p. 384). Following the instructions in the [datacamp tutorial](https://www.datacamp.com/community/tutorials/hierarchical-clustering-R), I scaled my three vectors, performed the hierarchical cluster analysis, and measured the goodness of clusters. 
6. I interpreted the results. 3 out of 8 students were not in their expected clusters. Despite high MTLD and AWL, two students were rated as Intermediate , although they were advanced. Despite low MTLD and AWL, one student was rated as advanced, although they were intermediate. Why have Students 3, 2, and 7 been chosen for one cluster? The review of raw numbers shows that Students 3, 2, 7 have the lowest MTLD and AWL. Lexical density was not considered in clustering. 
I have three possible interpretations. First, LC measures do not influence proficiency ratings at intermediate and advanced levels. It is possible that syntactic complexity measures, or discourse and strategic competences are more important at more advanced levels of profociency. Second, LC measures relevant for English texts may not be relevant for Russian texts. In different languages, we may need to identify different LC measures corresponding to growth in proficiency. Third, these lexical complexity measures may be credible and it is proficiency ratings which are flawed. The third interpretation is possible but highly unlikely since proficiency ratings are more holistic measures which encompass different writing competencies, while lexical complexity measures are just one construct with the grammatical competency of writing ability. 
At the same time, it is difficult to come up with generalizable results since there are many study limitations. The corpus size is too small. The tokenization rules should be checked once more. The length of essays which was not controlled in this paper might have skewed the results since there was one student who has written three times less than other students in the sub-corpus. The division into lexical and non-lexical items may be revised in the future. I need to read more about non-lexical items in order to be able to explain why pronouns should be also considered non-lexical items in Russian. MTLD should be compared with other TTR measures. While AWL and density are ratios, MTLD is calculated using a formula which covers these numbers with a layer of mysteriousness.  

## Conclusions
Overall, I am glad I worked on this project. I realized that I can code and I can understand how magic LC numbers are calculated. In the future, I see myself trying to work out how to calculate lexical bundles and syntactic complexity measures and assess how relevant they are for proficiency ratings.  

## References




